{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 923,
          "sourceType": "datasetVersion",
          "datasetId": 453
        },
        {
          "sourceId": 10679236,
          "sourceType": "datasetVersion",
          "datasetId": 6615525
        },
        {
          "sourceId": 10679899,
          "sourceType": "datasetVersion",
          "datasetId": 6616012
        }
      ],
      "dockerImageVersionId": 30839,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import librosa\n",
        "import librosa.display\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "metadata": {
        "id": "Ps7elGBf9H_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define the dataset directory\n",
        "data_dir = '/content/drive/MyDrive/PhysioNet2021/Data'\n",
        "classes = ['Abnormal', 'Normal']  # Define class labels\n",
        "\n",
        "# Function to load and preprocess audio files\n",
        "def load_and_preprocess_data(data_dir, classes, target_shape=(128, 128)):\n",
        "    data, labels = [], []\n",
        "\n",
        "    for i, class_name in enumerate(classes):\n",
        "        class_path = os.path.join(data_dir, class_name)  # Adjust for nested structure\n",
        "        for filename in os.listdir(class_path):\n",
        "            if filename.endswith('.wav'):\n",
        "                file_path = os.path.join(class_path, filename)\n",
        "\n",
        "                # Load audio file\n",
        "                audio_data, sample_rate = librosa.load(file_path, sr=None)\n",
        "\n",
        "                # Convert to Mel spectrogram\n",
        "                mel_spectrogram = librosa.feature.melspectrogram(y=audio_data, sr=sample_rate)\n",
        "                mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
        "\n",
        "                # Resize the spectrogram\n",
        "                resized_spectrogram = cv2.resize(mel_spectrogram_db, target_shape)\n",
        "\n",
        "                # Convert grayscale to RGB-like format\n",
        "                spectrogram_rgb = np.stack([resized_spectrogram] * 3, axis=-1)\n",
        "\n",
        "                data.append(spectrogram_rgb)\n",
        "                labels.append(i)\n",
        "\n",
        "    return np.array(data), np.array(labels)\n",
        "\n",
        "# Load the dataset\n",
        "data, labels = load_and_preprocess_data(data_dir, classes)\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "labels = to_categorical(labels, num_classes=len(classes))"
      ],
      "metadata": {
        "id": "P9P9kW1580Ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "best_f1 = 0\n",
        "best_model = None\n",
        "best_history = None\n",
        "best_y_true = None\n",
        "best_y_pred = None\n",
        "best_fold = None\n",
        "fold_accuracies = []\n",
        "fold_precisions = []\n",
        "fold_recalls = []\n",
        "fold_f1s = []\n",
        "\n",
        "fold = 1\n",
        "for train_index, test_index in skf.split(data, np.argmax(labels, axis=1)):\n",
        "    print(f\"\\nTraining Fold {fold}...\")\n",
        "\n",
        "    X_train, X_test = data[train_index], data[test_index]\n",
        "    y_train, y_test = labels[train_index], labels[test_index]\n",
        "\n",
        "    X_train = preprocess_input(X_train)\n",
        "    X_test = preprocess_input(X_test)\n",
        "\n",
        "    # Load Pretrained GoogleNet (InceptionV3)\n",
        "    base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "\n",
        "    # Freeze initial layers (fine-tuning later)\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # Add Custom Layers on top of GoogleNet\n",
        "    x = GlobalAveragePooling2D()(base_model.output)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    output_layer = Dense(len(classes), activation='softmax')(x)\n",
        "\n",
        "    # Create the model\n",
        "    model = Model(inputs=base_model.input, outputs=output_layer)\n",
        "\n",
        "    classifier = Model(inputs=base_model.output, outputs=output_layer)\n",
        "    plot_model(classifier, to_file='inceptionv3_top_only.png', show_shapes=True)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Callbacks for early stopping & learning rate decay\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
        "\n",
        "    # Train the model\n",
        "    history=model.fit(X_train, y_train,\n",
        "              epochs=50,\n",
        "              batch_size=32,\n",
        "              validation_data=(X_test, y_test),\n",
        "              callbacks=[early_stopping, reduce_lr], verbose=0)\n",
        "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='weighted')\n",
        "    recall = recall_score(y_true, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "    print(f\"Fold {fold} Weighted F1-score: {f1:.4f}\")\n",
        "    fold_accuracies.append(accuracy)\n",
        "    fold_precisions.append(precision)\n",
        "    fold_recalls.append(recall)\n",
        "    fold_f1s.append(f1)\n",
        "\n",
        "    # Save best fold\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_model = model\n",
        "        best_history = history\n",
        "        best_y_pred = y_pred\n",
        "        best_y_true = y_true\n",
        "\n",
        "    fold += 1\n",
        "\n",
        "# === After all folds === #\n",
        "print(\"\\n Cross-validation complete.\")\n",
        "print(\"=== Average Metrics Across Folds ===\")\n",
        "print(f\"Accuracy: {np.mean(fold_accuracies):.4f}\")\n",
        "print(f\"Precision: {np.mean(fold_precisions):.4f}\")\n",
        "print(f\"Recall: {np.mean(fold_recalls):.4f}\")\n",
        "print(f\"F1 Score: {np.mean(fold_f1s):.4f}\")\n",
        "\n",
        "# === Best Fold Results === #\n",
        "print(\"\\n Classification Report (Best Fold):\")\n",
        "print(classification_report(best_y_true, best_y_pred, target_names=classes))\n",
        "\n",
        "print(\" Confusion Matrix (Best Fold):\")\n",
        "cm = confusion_matrix(best_y_true, best_y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix â€“ Best Fold\")\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uQSDrtFSeJZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('inceptionv3_model.keras')"
      ],
      "metadata": {
        "id": "8wFoezbQPe1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def plot_training_history(history):\n",
        "    \"\"\"\n",
        "    Plots training and validation loss & accuracy from a Keras model history object.\n",
        "\n",
        "    Args:\n",
        "        history: Keras History object from model.fit()\n",
        "    \"\"\"\n",
        "    # Create subplots for Loss and Accuracy\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "    # Loss Plot\n",
        "    ax[0].plot(history.history['loss'], label='Training Loss', color='blue')\n",
        "    ax[0].plot(history.history['val_loss'], label='Validation Loss', color='red')\n",
        "    ax[0].set_xlabel('Epochs')\n",
        "    ax[0].set_ylabel('Loss')\n",
        "    ax[0].set_title('Training & Validation Loss')\n",
        "    ax[0].legend()\n",
        "\n",
        "    # Accuracy Plot\n",
        "    ax[1].plot(history.history['accuracy'], label='Training Accuracy', color='blue')\n",
        "    ax[1].plot(history.history['val_accuracy'], label='Validation Accuracy', color='red')\n",
        "    ax[1].set_xlabel('Epochs')\n",
        "    ax[1].set_ylabel('Accuracy')\n",
        "    ax[1].set_title('Training & Validation Accuracy')\n",
        "    ax[1].legend()\n",
        "\n",
        "    # Show the plots\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-06T14:11:07.666346Z",
          "iopub.execute_input": "2025-02-06T14:11:07.666682Z",
          "iopub.status.idle": "2025-02-06T14:11:07.672556Z",
          "shell.execute_reply.started": "2025-02-06T14:11:07.666650Z",
          "shell.execute_reply": "2025-02-06T14:11:07.671767Z"
        },
        "id": "Lctg5S0YMP5k"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training performance\n",
        "plot_training_history(history)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-06T14:14:05.801607Z",
          "iopub.execute_input": "2025-02-06T14:14:05.801929Z",
          "iopub.status.idle": "2025-02-06T14:14:06.161980Z",
          "shell.execute_reply.started": "2025-02-06T14:14:05.801905Z",
          "shell.execute_reply": "2025-02-06T14:14:06.161107Z"
        },
        "id": "AYJAUw_MMP5n"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**YamNet**"
      ],
      "metadata": {
        "id": "XvmWpuQmMP5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-io"
      ],
      "metadata": {
        "trusted": true,
        "id": "nNY__sR3MP5o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8338ff35-fcd8-49c2-fc4f-ea400b0881ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-io in /usr/local/lib/python3.11/dist-packages (0.37.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.37.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow-io) (0.37.1)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import librosa\n",
        "import numpy as np\n",
        "import tensorflow_io as tfio\n",
        "\n",
        "# Load YAMNet Pretrained Model\n",
        "yamnet_model = hub.load('https://tfhub.dev/google/yamnet/1')\n",
        "\n",
        "# Function to Convert Audio to YAMNet Features\n",
        "def extract_yamnet_features(file_path):\n",
        "    audio_data, sample_rate = librosa.load(file_path, sr=16000)  # Resample to 16kHz\n",
        "    waveform = tf.convert_to_tensor(audio_data, dtype=tf.float32)\n",
        "\n",
        "    # Run YAMNet to get embeddings\n",
        "    scores, embeddings, spectrogram = yamnet_model(waveform)\n",
        "\n",
        "    return embeddings.numpy()  # Convert to NumPy array\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-06T14:14:10.886873Z",
          "iopub.execute_input": "2025-02-06T14:14:10.887180Z",
          "iopub.status.idle": "2025-02-06T14:14:14.043385Z",
          "shell.execute_reply.started": "2025-02-06T14:14:10.887157Z",
          "shell.execute_reply": "2025-02-06T14:14:14.042665Z"
        },
        "id": "0I97v4YxMP5o",
        "collapsed": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/PhysioNet2021/Data'\n",
        "classes = ['Abnormal', 'Normal']\n",
        "\n",
        "X, y = [], []\n",
        "\n",
        "for class_idx, class_name in enumerate(classes):\n",
        "    class_path = os.path.join(data_dir, class_name)  # Adjust for nested folder structure\n",
        "    for filename in os.listdir(class_path):\n",
        "        if filename.endswith('.wav'):\n",
        "            file_path = os.path.join(class_path, filename)\n",
        "\n",
        "            # Extract YAMNet embeddings\n",
        "            embedding = extract_yamnet_features(file_path)\n",
        "\n",
        "            X.append(embedding.mean(axis=0))  # Take the mean of embeddings (feature vector)\n",
        "            y.append(class_idx)  # Assign class label\n",
        "\n",
        "# Convert to NumPy arrays\n",
        "X = np.array(X)\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=len(classes))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-06T14:14:15.425542Z",
          "iopub.execute_input": "2025-02-06T14:14:15.425894Z",
          "iopub.status.idle": "2025-02-06T14:14:52.523250Z",
          "shell.execute_reply.started": "2025-02-06T14:14:15.425867Z",
          "shell.execute_reply": "2025-02-06T14:14:52.522417Z"
        },
        "id": "Vn6Y5UhxMP5o"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "best_f1 = 0\n",
        "best_model = None\n",
        "best_history = None\n",
        "best_y_true = None\n",
        "best_y_pred = None\n",
        "best_fold = None\n",
        "fold_accuracies = []\n",
        "fold_precisions = []\n",
        "fold_recalls = []\n",
        "fold_f1s = []\n",
        "\n",
        "fold = 1\n",
        "for train_index, test_index in skf.split(X, np.argmax(y, axis=1)):\n",
        "    print(f\"\\nTraining Fold {fold}...\")\n",
        "\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X.shape[1],)),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(len(classes), activation='softmax')  # Output layer\n",
        "])\n",
        "\n",
        "    plot_model(model, to_file='yamnet.png', show_shapes=True, show_layer_names=True)\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(learning_rate=0.0005),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "\n",
        "    # Callbacks for early stopping & learning rate decay\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
        "\n",
        "    # Train the model\n",
        "    history=model.fit(X_train, y_train,\n",
        "              epochs=50,\n",
        "              batch_size=32,\n",
        "              validation_data=(X_test, y_test),\n",
        "              callbacks=[early_stopping, reduce_lr], verbose=0)\n",
        "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='weighted')\n",
        "    recall = recall_score(y_true, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "    print(f\"Fold {fold} Weighted F1-score: {f1:.4f}\")\n",
        "    fold_accuracies.append(accuracy)\n",
        "    fold_precisions.append(precision)\n",
        "    fold_recalls.append(recall)\n",
        "    fold_f1s.append(f1)\n",
        "\n",
        "    # Save best fold\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_model = model\n",
        "        best_history = history\n",
        "        best_y_pred = y_pred\n",
        "        best_y_true = y_true\n",
        "\n",
        "    fold += 1\n",
        "\n",
        "# === After all folds === #\n",
        "print(\"\\n Cross-validation complete.\")\n",
        "print(\"=== Average Metrics Across Folds ===\")\n",
        "print(f\"Accuracy: {np.mean(fold_accuracies):.4f}\")\n",
        "print(f\"Precision: {np.mean(fold_precisions):.4f}\")\n",
        "print(f\"Recall: {np.mean(fold_recalls):.4f}\")\n",
        "print(f\"F1 Score: {np.mean(fold_f1s):.4f}\")\n",
        "\n",
        "# === Best Fold Results === #\n",
        "print(\"\\n Classification Report (Best Fold):\")\n",
        "print(classification_report(best_y_true, best_y_pred, target_names=classes))\n",
        "\n",
        "print(\" Confusion Matrix (Best Fold):\")\n",
        "cm = confusion_matrix(best_y_true, best_y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix â€“ Best Fold\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rdcA2b8ZufsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('yamnet_model.keras')"
      ],
      "metadata": {
        "id": "WBJadm1ucTRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training performance\n",
        "plot_training_history(history)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-06T14:15:43.293029Z",
          "iopub.execute_input": "2025-02-06T14:15:43.293314Z",
          "iopub.status.idle": "2025-02-06T14:15:43.662233Z",
          "shell.execute_reply.started": "2025-02-06T14:15:43.293290Z",
          "shell.execute_reply": "2025-02-06T14:15:43.661228Z"
        },
        "id": "Rjgfin9eMP5q"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Load VGGish Pretrained Model from TensorFlow Hub\n",
        "vggish_model = hub.load(\"https://tfhub.dev/google/vggish/1\")\n",
        "\n",
        "# Function to Extract VGGish Features from Audio\n",
        "def extract_vggish_features(file_path):\n",
        "    # Load the audio file and resample to 16kHz (VGGish requires 16kHz input)\n",
        "    audio_data, sr = librosa.load(file_path, sr=16000)\n",
        "\n",
        "    # Convert to Tensor\n",
        "    waveform = tf.convert_to_tensor(audio_data, dtype=tf.float32)\n",
        "\n",
        "    # Run VGGish to get audio embeddings\n",
        "    embeddings = vggish_model(waveform)\n",
        "\n",
        "    return embeddings.numpy()  # Convert to NumPy array"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-06T14:15:59.207946Z",
          "iopub.execute_input": "2025-02-06T14:15:59.208240Z",
          "iopub.status.idle": "2025-02-06T14:16:02.572185Z",
          "shell.execute_reply.started": "2025-02-06T14:15:59.208219Z",
          "shell.execute_reply": "2025-02-06T14:16:02.571491Z"
        },
        "id": "iEamvVQiMP5q"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dataset directory\n",
        "data_dir = '/content/drive/MyDrive/PhysioNet2021/Data'\n",
        "classes = ['Abnormal', 'Normal']\n",
        "\n",
        "X, y = [], []\n",
        "\n",
        "# Loop through dataset and extract VGGish features\n",
        "for class_idx, class_name in enumerate(classes):\n",
        "    class_path = os.path.join(data_dir, class_name)  # Adjust for nested structure\n",
        "    for filename in os.listdir(class_path):\n",
        "        if filename.endswith('.wav'):\n",
        "            file_path = os.path.join(class_path, filename)\n",
        "\n",
        "            # Extract VGGish embeddings\n",
        "            embedding = extract_vggish_features(file_path)\n",
        "\n",
        "            # Store features\n",
        "            X.append(embedding.mean(axis=0))  # Average embeddings for a single vector\n",
        "            y.append(class_idx)\n",
        "\n",
        "# Convert to NumPy arrays\n",
        "X = np.array(X)\n",
        "y = to_categorical(y, num_classes=len(classes))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-06T14:16:15.463731Z",
          "iopub.execute_input": "2025-02-06T14:16:15.464034Z",
          "iopub.status.idle": "2025-02-06T14:17:32.872992Z",
          "shell.execute_reply.started": "2025-02-06T14:16:15.464009Z",
          "shell.execute_reply": "2025-02-06T14:17:32.872022Z"
        },
        "id": "6W7uTwoDMP5q"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "best_f1 = 0\n",
        "best_model = None\n",
        "best_history = None\n",
        "best_y_true = None\n",
        "best_y_pred = None\n",
        "best_fold = None\n",
        "fold_accuracies = []\n",
        "fold_precisions = []\n",
        "fold_recalls = []\n",
        "fold_f1s = []\n",
        "\n",
        "fold = 1\n",
        "for train_index, test_index in skf.split(X, np.argmax(y, axis=1)):\n",
        "    print(f\"\\nTraining Fold {fold}...\")\n",
        "\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X.shape[1],)),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(len(classes), activation='softmax')  # Output layer for classification\n",
        "])\n",
        "\n",
        "    plot_model(model, to_file='vggish.png', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(learning_rate=0.0005),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "\n",
        "    # Callbacks for early stopping & learning rate decay\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
        "\n",
        "    # Train the model\n",
        "    history=model.fit(X_train, y_train,\n",
        "              epochs=50,\n",
        "              batch_size=32,\n",
        "              validation_data=(X_test, y_test),\n",
        "              callbacks=[early_stopping, reduce_lr], verbose=0)\n",
        "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='weighted')\n",
        "    recall = recall_score(y_true, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "    print(f\"Fold {fold} Weighted F1-score: {f1:.4f}\")\n",
        "    fold_accuracies.append(accuracy)\n",
        "    fold_precisions.append(precision)\n",
        "    fold_recalls.append(recall)\n",
        "    fold_f1s.append(f1)\n",
        "\n",
        "    # Save best fold\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_model = model\n",
        "        best_history = history\n",
        "        best_y_pred = y_pred\n",
        "        best_y_true = y_true\n",
        "\n",
        "    fold += 1\n",
        "\n",
        "# === After all folds === #\n",
        "print(\"\\n Cross-validation complete.\")\n",
        "print(\"=== Average Metrics Across Folds ===\")\n",
        "print(f\"Accuracy: {np.mean(fold_accuracies):.4f}\")\n",
        "print(f\"Precision: {np.mean(fold_precisions):.4f}\")\n",
        "print(f\"Recall: {np.mean(fold_recalls):.4f}\")\n",
        "print(f\"F1 Score: {np.mean(fold_f1s):.4f}\")\n",
        "\n",
        "# === Best Fold Results === #\n",
        "print(\"\\n Classification Report (Best Fold):\")\n",
        "print(classification_report(best_y_true, best_y_pred, target_names=classes))\n",
        "\n",
        "print(\" Confusion Matrix (Best Fold):\")\n",
        "cm = confusion_matrix(best_y_true, best_y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix â€“ Best Fold\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "e9gtAoYtycY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('vggish_model.keras')"
      ],
      "metadata": {
        "id": "0tpnJB4dcYJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training_history(history)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-06T14:23:28.477127Z",
          "iopub.execute_input": "2025-02-06T14:23:28.477422Z",
          "iopub.status.idle": "2025-02-06T14:23:28.848587Z",
          "shell.execute_reply.started": "2025-02-06T14:23:28.477398Z",
          "shell.execute_reply": "2025-02-06T14:23:28.847768Z"
        },
        "id": "ggRQhw5EMP5r"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('audio_classification_model.keras')"
      ],
      "metadata": {
        "id": "pjbP7VQ-tXBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ATTEMPT**"
      ],
      "metadata": {
        "id": "GBpQaIGbMP5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained model\n",
        "from tensorflow.keras.models import load_model\n",
        "model_path = \"/content/audio_classification_model.keras\"  # Path where the model is saved\n",
        "model.save(model_path)  # Save the trained model\n",
        "model = load_model(model_path)\n",
        "\n",
        "# Function to classify a new audio file\n",
        "def classify_audio(file_path, model):\n",
        "    # Extract VGGish features from the new audio file\n",
        "    embedding = extract_vggish_features(file_path)\n",
        "\n",
        "    # Average embeddings to match training input shape\n",
        "    embedding = np.mean(embedding, axis=0).reshape(1, -1)  # Reshape for model input\n",
        "\n",
        "    # Make a prediction\n",
        "    prediction_probs = model.predict(embedding)  # Get class probabilities\n",
        "    predicted_class = np.argmax(prediction_probs)  # Get predicted class index\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Predicted Class: {classes[predicted_class]}\")\n",
        "    print(f\"Class Probabilities: {prediction_probs}\")\n",
        "\n",
        "    return predicted_class, prediction_probs\n",
        "\n",
        "# Test on a new unseen audio file\n",
        "test_audio_file = \"/content/c0002.wav\"  # Replace with actual file path\n",
        "predicted_class, prediction_probs = classify_audio(test_audio_file, model)\n"
      ],
      "metadata": {
        "id": "8bTg9gWEqQhO",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Function to extract Mel spectrogram features\n",
        "def extract_mel_spectrogram(file_path, sr=16000, n_mels=64, hop_length=512):\n",
        "    # Load audio file and resample to 16kHz\n",
        "    audio_data, _ = librosa.load(file_path, sr=sr)\n",
        "\n",
        "    # Compute Mel spectrogram\n",
        "    mel_spectrogram = librosa.feature.melspectrogram(y=audio_data, sr=sr, n_mels=n_mels, hop_length=hop_length)\n",
        "\n",
        "    # Convert to log scale (better for CNNs)\n",
        "    mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
        "\n",
        "    return mel_spectrogram\n",
        "\n",
        "# Example Usage:\n",
        "file_path = \"/content/c0002.wav\"\n",
        "mel_spec = extract_mel_spectrogram(file_path)\n",
        "\n",
        "# Display Mel Spectrogram\n",
        "plt.figure(figsize=(10, 4))\n",
        "librosa.display.specshow(mel_spec, sr=16000, hop_length=512, cmap='viridis')\n",
        "plt.colorbar(label='dB')\n",
        "plt.title(\"Mel Spectrogram for Abnormal\")\n",
        "plt.show()\n",
        "\n",
        "# Example Usage:\n",
        "file_path = \"/content/c0003.wav\"\n",
        "mel_spec = extract_mel_spectrogram(file_path)\n",
        "\n",
        "# Display Mel Spectrogram\n",
        "plt.figure(figsize=(10, 4))\n",
        "librosa.display.specshow(mel_spec, sr=16000, hop_length=512, cmap='viridis')\n",
        "plt.colorbar(label='dB')\n",
        "plt.title(\"Mel Spectrogram for Normal\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-06T14:31:26.958186Z",
          "iopub.execute_input": "2025-02-06T14:31:26.958491Z",
          "iopub.status.idle": "2025-02-06T14:31:27.416685Z",
          "shell.execute_reply.started": "2025-02-06T14:31:26.958467Z",
          "shell.execute_reply": "2025-02-06T14:31:27.415743Z"
        },
        "id": "yvrHCpCCMP5r",
        "collapsed": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}